import { streamText, smoothStream, createDataStreamResponse, streamObject, generateObject } from 'ai';
import { createClient } from '@/utils/supabase/server';
import { providers } from '@/lib/providers';
import { getModelById} from '@/lib/models/config';
import { MultiModalMessage } from './types';
import { z } from 'zod';
// import { markdownJoinerTransform } from 'markdown-transform';
import { 
  handlePromptShortcuts,
  saveUserMessage,
  createOrUpdateAssistantMessage,
  handleStreamCompletion,
  buildSystemPrompt
} from './services/chatService';
import { 
  generateMessageId, 
  convertMessageForAI, 
  validateAndUpdateSession,
  getProviderFromModel,
  convertMultiModalToMessage,
  selectMessagesWithinTokenLimit,
  detectImages,
  detectPDFs,
  detectCodeAttachments
} from './utils/messageUtils';
import { 
  createWebSearchTool, 
  createJinaLinkReaderTool, 
  createImageGeneratorTool, 
  createCalculatorTool, 
  createAcademicSearchTool, 
  // createXSearchTool, 
  createYouTubeSearchTool, 
  createYouTubeLinkAnalyzerTool, 
} from './tools';
// üÜï Import both rate limiting functions
import { handleRateLimiting, handleChatflixRateLimiting } from './utils/ratelimit';
// import { toolPrompts } from './prompts/toolPrompts';
import { checkSubscription } from '@/lib/polar';

// Î©îÎ™®Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏ ÏµúÏ†ÅÌôî Í¥ÄÎ†® ÏÉÅÏàò
const MEMORY_UPDATE_THRESHOLD = 30 * 60 * 1000; // 30Î∂Ñ Í∞ÑÍ≤©

// ÎπÑÍµ¨ÎèÖÏûê Ïª®ÌÖçÏä§Ìä∏ ÏúàÎèÑÏö∞ Ï†úÌïú
const CONTEXT_WINDOW_LIMIT_NON_SUBSCRIBER = 60000; // 60K tokens

// Î©îÎ™®Î¶¨ Í¥ÄÎ†® import
import { initializeMemoryBank, getAllMemoryBank, getLastMemoryUpdate } from '@/utils/memory-bank';
import { updateAllMemoryBanks, smartUpdateMemoryBanks } from './services/memoryService';
import { estimateTokenCount } from '@/utils/context-manager';
import { selectOptimalModel, estimateMultiModalTokens } from './services/modelSelector';
import { 
  analyzeRequestAndDetermineRoute,
  analyzeContextRelevance 
} from './services/analysisService';
import { markdownJoinerTransform } from './markdown-transform';


// Helper function to increment user daily request count
async function incrementSuccessfulRequestCount(
  supabaseClient: any,
  userId: string,
  requestDate: string,
  currentCount: number,
  isUserSubscribed: boolean
) {
  try {
    await supabaseClient
      .from('user_daily_requests')
      .upsert({
        user_id: userId,
        date: requestDate,
        count: currentCount + 1,
        last_request_at: new Date().toISOString(),
        is_subscribed: isUserSubscribed
      }, {
        onConflict: 'user_id,date' // This ensures that if a record for the user and date already exists, it's updated.
      });
  } catch (error) {
    // console.error('Failed to update successful request count:', error);
  }
}

// Helper function to collect tool results from various tools
function collectToolResults(tools: Record<string, any>, toolNames: string[]): any {
  const collectedToolResults: any = {};
  
  toolNames.forEach((toolName: string) => {
    switch (toolName) {
      case 'calculator':
        if (tools.calculator?.calculationSteps?.length > 0) {
          collectedToolResults.calculationSteps = tools.calculator.calculationSteps;
        }
        break;
      case 'web_search':
        if (tools.web_search?.searchResults?.length > 0) {
          collectedToolResults.webSearchResults = tools.web_search.searchResults;
        }
        break;
      case 'link_reader':
        if (tools.link_reader?.linkAttempts?.length > 0) {
          collectedToolResults.linkReaderAttempts = tools.link_reader.linkAttempts;
        }
        break;
      case 'image_generator':
        if (tools.image_generator?.generatedImages?.length > 0) {
          collectedToolResults.generatedImages = tools.image_generator.generatedImages;
        }
        break;
      case 'academic_search':
        if (tools.academic_search?.searchResults?.length > 0) {
          collectedToolResults.academicSearchResults = tools.academic_search.searchResults;
        }
        break;
      case 'youtube_search':
        if (tools.youtube_search?.searchResults?.length > 0) {
          collectedToolResults.youtubeSearchResults = tools.youtube_search.searchResults;
        }
        break;
      case 'youtube_link_analyzer':
        if (tools.youtube_link_analyzer?.analysisResults?.length > 0) {
          collectedToolResults.youtubeLinkAnalysisResults = tools.youtube_link_analyzer.analysisResults;
        }
        break;
    }
  });
  
  return collectedToolResults;
}

async function generateFollowUpQuestions(
  userQuery: string,
  aiResponse: string,
  responseType: 'text' | 'file' = 'text'
): Promise<string[]> {
  try {
    const contextInfo = responseType === 'file' ? 
      'The AI has generated files/documents for the user.' : 
      'The AI has provided a text response to the user.';
    
    const followUpResult = await generateObject({
      model: providers.languageModel('gemini-2.0-flash'),
      prompt: `You are generating follow-up questions that a USER would naturally ask or input to an AI assistant. These should be direct requests, commands, or questions that users would actually type, NOT questions the AI would ask the user.

**CRITICAL INSTRUCTION: Generate user inputs TO the AI, not AI questions TO the user**

User's original query: "${userQuery}"
AI's response: "${aiResponse}"
Context: ${contextInfo}

**WRONG EXAMPLES (AI asking user - DO NOT generate these):**
‚ùå "What details would you like me to emphasize in this image?"
‚ùå "Which style would you prefer?"
‚ùå "Do you want me to modify anything?"
‚ùå "Would you like me to create variations?"

**CORRECT EXAMPLES (User asking/requesting from AI - Generate these types):**
‚úÖ "Create a similar image with a dog instead"
‚úÖ "Generate a complete code file for this project"
‚úÖ "Search for the latest news about this topic"
‚úÖ "How does this algorithm work?"
‚úÖ "What are the pros and cons of this approach?"
‚úÖ "Make this image in a different style"
‚úÖ "Find research papers about this subject"
‚úÖ "Create a detailed documentation file"
‚úÖ "Search YouTube for tutorials on this"

**Generate 3 different types of user inputs:**
1. **Action Request**: User asks AI to create, generate, search, or make something
2. **Information Question**: User asks AI to explain, analyze, or provide information
3. **Follow-up Inquiry**: User asks about alternatives, improvements, or related topics

**IMPORTANT RULES:**
- Write as natural user inputs TO the AI (commands, requests, or questions)
- Can be imperative ("Create...") or interrogative ("How does...?", "What is...?")
- Respond in the same language as the user's original query
- Make them natural and actionable - things users would actually type
- Each input should be distinctly different in purpose`,
      schema: z.object({
        followup_questions: z.array(z.string()).length(3)
      })
    });
    
    return followUpResult.object.followup_questions;
  } catch (e) { 
    return [];
  }
}

// Tool initialization helper function
function initializeTool(type: string, dataStream: any) {
  switch (type) {
    case 'web_search':
      return createWebSearchTool(dataStream);
    case 'calculator':
      return createCalculatorTool(dataStream);
    case 'link_reader':
      return createJinaLinkReaderTool(dataStream);
    case 'image_generator':
      return createImageGeneratorTool(dataStream);
    case 'academic_search':
      return createAcademicSearchTool(dataStream);
    // case 'x_search':
    //   return createXSearchTool(dataStream);
    case 'youtube_search':
      return createYouTubeSearchTool(dataStream);
    case 'youtube_link_analyzer':
      return createYouTubeLinkAnalyzerTool(dataStream);
    default:
      throw new Error(`Unknown tool type: ${type}`);
  }
}

export async function POST(req: Request) {
    const supabase = await createClient();
    const { data: { user }, error: userError } = await supabase.auth.getUser();
    
    if (userError || !user) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    const requestData = await req.json();
    let { messages, model, chatId, isRegeneration, existingMessageId, saveToDb = true, isAgentEnabled = false } = requestData;

    // Map Chatflix Ultimate model to appropriate model based on agent mode
    if (model === 'chatflix-ultimate' || model === 'chatflix-ultimate-pro') {
        // Store the original model name for DB storage
        requestData.originalModel = model;
        
        try {
          const modelType = model as 'chatflix-ultimate' | 'chatflix-ultimate-pro';
          const { selectedModel } = await selectOptimalModel(messages, modelType);
          model = selectedModel;
        } catch (error) {
          // console.error('Error in model selection:', error);
          // Ïò§Î•ò Î∞úÏÉù Ïãú Í∏∞Î≥∏ Î™®Îç∏ ÏÇ¨Ïö©
          model = 'gemini-2.5-pro';
        }
      }

    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return new Response(JSON.stringify({ error: 'Invalid messages format' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    // Íµ¨ÎèÖ ÏÉÅÌÉú ÌôïÏù∏
    const isSubscribed = await checkSubscription(user.id);
    
    // ÏÇ¨Ïö©ÏûêÏùò Ïò§Îäò ÏöîÏ≤≠ ÌöüÏàò ÌôïÏù∏
    const today = new Date().toISOString().split('T')[0]; // YYYY-MM-DD ÌòïÏãù
    const { data: userRequests, error: requestsError } = await supabase
      .from('user_daily_requests')
      .select('count')
      .eq('user_id', user.id)
      .eq('date', today)
      .single();
    
    // ÌòÑÏû¨ ÏöîÏ≤≠ ÌöüÏàò (ÏóÜÏúºÎ©¥ 0ÏúºÎ°ú ÏãúÏûë)
    const currentRequestCount = userRequests?.count || 0;
  
    // üÜï Handle rate limiting based on model type
    const originalModel = requestData.originalModel;
    const isChatflixModel = originalModel === 'chatflix-ultimate' || originalModel === 'chatflix-ultimate-pro';
    
    if (isChatflixModel) {
      // Chatflix Î™®Îç∏ÏùÄ ÏûêÏ≤¥ rate limitÎßå Ï≤¥ÌÅ¨ (ÏÑ†ÌÉùÎêú Í∞úÎ≥Ñ Î™®Îç∏ rate limit Î¨¥Ïãú)
      const chatflixRateLimitResult = await handleChatflixRateLimiting(user.id, originalModel);
      if (!chatflixRateLimitResult.success) {
        const { error } = chatflixRateLimitResult;
        
        if (error) {
          return new Response(JSON.stringify({
            error: 'Too many requests',
            message: error.message,
            retryAfter: error.retryAfter,
            reset: new Date(error.reset).toISOString(),
            limit: error.limit,
            level: error.level,
            model: originalModel // Use original Chatflix model name
          }), {
            status: 429,
            headers: {
              'Content-Type': 'application/json',
              'X-RateLimit-Limit': error.limit.toString(),
              'X-RateLimit-Remaining': '0',
              'X-RateLimit-Reset': new Date(error.reset).toISOString(),
            }
          });
        }
      }
    } else {
      // ÏùºÎ∞ò Î™®Îç∏ÏùÄ Í∏∞Ï°¥ Î°úÏßÅ ÏÇ¨Ïö©
      const rateLimitResult = await handleRateLimiting(user.id, model);
      if (!rateLimitResult.success) {
        const { error } = rateLimitResult;
        
        if (error) {
          return new Response(JSON.stringify({
            error: 'Too many requests',
            message: error.message,
            retryAfter: error.retryAfter,
            reset: new Date(error.reset).toISOString(),
            limit: error.limit,
            level: error.level,
            model: model
          }), {
            status: 429,
            headers: {
              'Content-Type': 'application/json',
              'X-RateLimit-Limit': error.limit.toString(),
              'X-RateLimit-Remaining': '0',
              'X-RateLimit-Reset': new Date(error.reset).toISOString(),
            }
          });
        } else {
          // Fallback in case error is undefined
          return new Response(JSON.stringify({
            error: 'Too many requests',
            message: 'Rate limit exceeded'
          }), {
            status: 429,
            headers: {
              'Content-Type': 'application/json'
            }
          });
        }
      }
    }

    return createDataStreamResponse({
      execute: async (dataStream) => {

          let sessionValidationPromise;
          if (chatId) {
            sessionValidationPromise = validateAndUpdateSession(supabase, chatId, user.id, messages);
          } else {
            sessionValidationPromise = Promise.resolve();
          }

          // Î©îÎ™®Î¶¨ Î±ÖÌÅ¨ Ï¥àÍ∏∞Ìôî (Agent Î™®Îìú Ïó¨Î∂ÄÏôÄ ÏÉÅÍ¥ÄÏóÜÏù¥)
          let memoryInitPromise = initializeMemoryBank(
            supabase, 
            user.id,
            // user Í∞ùÏ≤¥ Ï†ÑÏ≤¥ Ï†ÑÎã¨
            user
          ).catch(err => {
            // Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç ÏßÑÌñâ
          });
          
          // Process messages in parallel
          const processMessagesPromises = messages.map(async (msg) => {
            const converted = await convertMessageForAI(msg, model, supabase);
            return {
              id: msg.id,
              ...converted
            } as MultiModalMessage;
          });
          
          // üîß HIGH PRIORITY OPTIMIZATION: Î≥ëÎ†¨ Ï≤òÎ¶¨ ÌôïÎåÄ
          // Wait for message processing and memory initialization in parallel
          const [
            processMessages
          ] = await Promise.all([
            Promise.all(processMessagesPromises),
            memoryInitPromise
          ]);
          
          // Process last message shortcut if needed
          const lastMessage = processMessages[processMessages.length - 1];
          const processedLastMessage = await handlePromptShortcuts(supabase, lastMessage, user.id) as MultiModalMessage;
          
          // Update the last message with processed shortcuts
          processMessages[processMessages.length - 1] = processedLastMessage;

          // Get memory data in parallel with other operations
          const memoryDataPromise = getAllMemoryBank(supabase, user.id);
          
          // Prepare DB operations (but don't wait)
          let dbOperationsPromise = Promise.resolve();
          
          if (lastMessage.role === 'user' && !isRegeneration && saveToDb && chatId) {
            dbOperationsPromise = new Promise(async (resolve) => {
              try {
                const { data: existingMessages } = await supabase
                  .from('messages')
                  .select('id')
                  .eq('chat_session_id', chatId)
                  .eq('user_id', user.id)
                  .order('created_at', { ascending: true })
                  .limit(2);
                
                const isInitialDbMessage = existingMessages?.length === 1;
                
                if (!isInitialDbMessage) {
                  await saveUserMessage(supabase, chatId, user.id, lastMessage, model);
                }
                resolve(undefined);
              } catch (error) {
                resolve(undefined);
              }
            });
          }

          const assistantMessageId = isRegeneration && existingMessageId 
            ? existingMessageId 
            : generateMessageId();

          if (chatId) {
            dbOperationsPromise = dbOperationsPromise.then(() => 
              createOrUpdateAssistantMessage(
                supabase,
                chatId,
                user.id,
                model,
                getProviderFromModel(model),
                isRegeneration,
                assistantMessageId
              )
            );
          }

          const abortController = new AbortController();

          const modelConfig = getModelById(model);
          const supportsReasoning = modelConfig?.reasoning?.enabled || false;
          // Get max output tokens for this model (if defined)
          // const maxOutputTokens = modelConfig?.maxOutputTokens;

          const providerOptions: any = {};

          if (supportsReasoning) {
            providerOptions.anthropic = { 
              thinking: { 
                type: 'enabled', 
                budgetTokens: 12000 
              } 
            };
            
            providerOptions.xai = { 
              reasoningEffort: 'high' 
            };
            
            providerOptions.openai = { 
              reasoningEffort: 'high',
            };
            
            providerOptions.google = { 
              thinkingConfig: { 
                thinkingBudget: 2048,
                includeThoughts: true // this shit doesnt work. don't know why.
              }, 
              safetySettings: [
                { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_NONE' },
                { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' },
                { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' },
                { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_NONE' },
              ],
            };
          }
          
          // Get memory data result
          const { data: memoryData } = await memoryDataPromise;
          
          // Î©îÎ™®Î¶¨ Î±ÖÌÅ¨ ÎÇ¥Ïö©Ïù¥ Ï¥àÍ∏∞Ìôî Í∞íÏù∏ÏßÄ ÌôïÏù∏
          const isDefaultMemory = memoryData && 
            memoryData.includes('This section contains basic information about the user') &&
            memoryData.includes('This section tracks user preferences such as UI style');
                    
          // 3. Ìñ•ÏÉÅÎêú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ (ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ Ïª®ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä)
          const currentSystemPrompt = buildSystemPrompt(
            isAgentEnabled ? 'agent' : 'regular',
            'TEXT_RESPONSE',
            // Ï¥àÍ∏∞ ÌÖúÌîåÎ¶øÏù∏ Í≤ΩÏö∞ÏóêÎäî ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ Ïª®ÌÖçÏä§Ìä∏Î•º Ï∂îÍ∞ÄÌïòÏßÄ ÏïäÏùå
            isDefaultMemory ? undefined : (memoryData || undefined)
          );
          
          // üîß MEDIUM PRIORITY OPTIMIZATION: ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÌÜ†ÌÅ∞ Í≥ÑÏÇ∞ Ìïú Î≤àÎßå ÏàòÌñâ
          const systemTokens = estimateTokenCount(currentSystemPrompt);
          const maxContextTokens = isSubscribed 
            ? (modelConfig?.contextWindow || 8000)
            : CONTEXT_WINDOW_LIMIT_NON_SUBSCRIBER;
          let remainingTokens = maxContextTokens - systemTokens;
          
          // üîß MEDIUM PRIORITY OPTIMIZATION: Î©îÏãúÏßÄÎ≥Ñ ÌÜ†ÌÅ∞ ÎØ∏Î¶¨ Í≥ÑÏÇ∞ Î∞è Ï∫êÏã±
          const messagesWithTokens = processMessages.map(msg => {
            const tokenCount = estimateMultiModalTokens(msg as any);
            return {
              ...msg,
              _tokenCount: tokenCount
            };
          });
          
          if (isAgentEnabled) {
            let contextFilter: any | null = null;
            
            // Re-calculate system tokens specifically for agent mode for accuracy
            const agentSystemPromptForCalc = buildSystemPrompt(
              'agent',
              'FILE_RESPONSE', // Use the potentially longest prompt for a safe calculation
              isDefaultMemory ? undefined : (memoryData || undefined)
            );
            const agentSystemTokens = estimateTokenCount(agentSystemPromptForCalc);
            remainingTokens = maxContextTokens - agentSystemTokens;

            const optimizedMessagesForRouting = selectMessagesWithinTokenLimit(
              messagesWithTokens, 
              remainingTokens,
            );

            // üîß HIGH PRIORITY OPTIMIZATION: Î©îÏãúÏßÄ Î≥ÄÌôò Ï§ëÎ≥µ Ï†úÍ±∞

            // ÌòÑÏû¨ ÏßàÎ¨∏ Ï∂îÏ∂úÏùÑ ÏúÑÌïú Ï§ÄÎπÑ
            let userQuery = '';
            
            // Í∞Å Î©îÏãúÏßÄÏóêÏÑú ÌÖçÏä§Ìä∏ Î∞è Ï≤®Î∂ÄÌååÏùº Ï†ïÎ≥¥ Ï∂îÏ∂ú Ìï®Ïàò
            const extractTextFromMessage = (msg: any) => {
              if (typeof msg.content === 'string') {
                return msg.content;
              } else if (Array.isArray(msg.content)) {
                // ÌÖçÏä§Ìä∏ Î∂ÄÎ∂Ñ Ï∂îÏ∂ú
                const textContent = msg.content
                  .filter((part: any) => part.type === 'text')
                  .map((part: any) => part.text)
                  .join('\n');
                
                // Ï≤®Î∂ÄÌååÏùº Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
                const attachmentInfo = [];
                
                // Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨
                const images = msg.content.filter((part: any) => part.type === 'image');
                if (images.length > 0) {
                  attachmentInfo.push(`[ATTACHED: ${images.length} image(s)]`);
                }
                
                // ÌååÏùº Ï≤òÎ¶¨
                const files = msg.content.filter((part: any) => part.type === 'file');
                files.forEach((file: any) => {
                  if (file.file) {
                    const fileName = file.file.name || '';
                    const fileType = file.file.contentType || '';
                    
                    // ÌååÏùº Ïú†ÌòïÏóê Îî∞Î•∏ Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥ Ï†úÍ≥µ
                    if (fileType.startsWith('image/') || fileName.match(/\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i)) {
                      attachmentInfo.push(`[ATTACHED: Image file - ${fileName}]`);
                    } else if (fileType === 'application/pdf' || fileName.toLowerCase().endsWith('.pdf')) {
                      attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
                    } else if (fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
                      const extension = fileName.split('.').pop();
                      attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
                    } else {
                      attachmentInfo.push(`[ATTACHED: File - ${fileName} (${fileType})]`);
                    }
                  }
                });
                
                // experimental_attachments Ï≤òÎ¶¨
                if (Array.isArray(msg.experimental_attachments)) {
                  msg.experimental_attachments.forEach((attachment: any) => {
                    const fileName = attachment.name || '';
                    const fileType = attachment.contentType || attachment.fileType || '';
                    
                    if (fileType === 'image' || fileType.startsWith('image/') || 
                        fileName.match(/\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i)) {
                      attachmentInfo.push(`[ATTACHED: Image file - ${fileName}]`);
                    } else if (fileType === 'pdf' || fileType === 'application/pdf' || 
                              fileName.toLowerCase().endsWith('.pdf')) {
                      attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
                    } else if (fileType === 'code' || 
                              fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
                      const extension = fileName.split('.').pop();
                      attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
                    } else if (fileName) {
                      attachmentInfo.push(`[ATTACHED: File - ${fileName} (${fileType})]`);
                    }
                  });
                }
                
                // ÌÖçÏä§Ìä∏ÏôÄ Ï≤®Î∂ÄÌååÏùº Ï†ïÎ≥¥ Í≤∞Ìï©
                if (textContent) {
                  return attachmentInfo.length > 0 
                    ? `${textContent}\n${attachmentInfo.join('\n')}` 
                    : textContent;
                } else if (attachmentInfo.length > 0) {
                  return attachmentInfo.join('\n');
                }
              }
              return '';
            };
            
            // ÌòÑÏû¨ ÏßàÎ¨∏Îßå userQueryÏóê Ìï†Îãπ
            const currentMessage = optimizedMessagesForRouting[optimizedMessagesForRouting.length - 1];
            userQuery = extractTextFromMessage(currentMessage);


            // üÜï STEP 0: Parallel Analysis - Context Relevance & Request Routing
            const hasToolResultsInHistory = messagesWithTokens.slice(0, -1).some(msg => 
              (msg as any).tool_results && 
              Object.keys((msg as any).tool_results).some(key => key !== 'token_usage')
            );
            const hasPreviousConversation = messagesWithTokens.length > 1;
            const shouldAnalyzeContext = hasPreviousConversation && hasToolResultsInHistory && messagesWithTokens.length > 3;

            // Define available tools list early for analysis
            let baseAvailableToolsList = [
              'web_search',
              'calculator',
              'link_reader',
              'image_generator',
              'academic_search',
              'youtube_search',
              'youtube_link_analyzer'
            ];

            const analysisModel = 'gemini-2.0-flash';

            // ÎèÑÍµ¨ ÏÑ§Î™Ö Í∞ùÏ≤¥ Ï†ïÏùò (Î∂ÑÏÑùÏóêÏÑú ÏÇ¨Ïö©)
            const toolDescriptions = {
              'web_search': 'Real-time information from the internet',
              'calculator': 'Mathematical calculations and computations',
              'link_reader': 'Reading and analyzing web page content',
              'image_generator': 'Creating visual content',
              'academic_search': 'Finding scholarly and research materials',
              'youtube_search': 'Finding relevant video content',
              'youtube_link_analyzer': 'Analyzing specific YouTube videos'
            };

            // üöÄ V6 Plan: New unified analysis and routing
            const [
              routeAnalysisResult,
              contextAnalysisResult
            ] = await Promise.all([
              analyzeRequestAndDetermineRoute(
                analysisModel,
                model,
                baseAvailableToolsList,
                convertMultiModalToMessage(messagesWithTokens, undefined), // Use all messages for routing analysis
                toolDescriptions
              ),
              shouldAnalyzeContext
                ? analyzeContextRelevance(analysisModel, convertMultiModalToMessage(messagesWithTokens, undefined))
                : Promise.resolve(null),
            ]);
            
            // Process context analysis results
            if (contextAnalysisResult) {
              try {
                contextFilter = contextAnalysisResult.object;
              } catch (error) {
                contextFilter = null;
              }
            }
            
            // V7 Optimization: Convert messages ONCE with the final context filter.
            const finalMessagesForAI = convertMultiModalToMessage(messagesWithTokens, contextFilter);
            
            // Recalculate token budget with the now-finalized message list.
            const messagesWithTokensFinal = finalMessagesForAI.map(msg => ({
              ...msg,
              _tokenCount: estimateMultiModalTokens(msg as any)
            }));
            
            const routingDecision = routeAnalysisResult.object;

            const hasImage = messagesWithTokens.some(msg => detectImages(msg));
            const hasFile = messagesWithTokens.some(msg => detectPDFs(msg) || detectCodeAttachments(msg));
            
            switch (routingDecision.route) {
              case 'CLARIFY':
                // Route to ask the user a clarifying question.
                const clarificationResult = streamText({
                  model: providers.languageModel('gemini-2.0-flash'),
                  experimental_transform: [
                    smoothStream({delayInMs: 25}),
                    markdownJoinerTransform(),
                  ],
                  system: `You are Chatflix, a friendly and helpful AI assistant. The user's request needs more information. Your task is to ask the user the clarifying question provided below in a natural, conversational way.

**Core Instruction: ALWAYS respond in the user's language.**

**Instructions:**
- Start with a brief, friendly acknowledgment.
- Then, ask the clarifying question naturally.
- Be conversational and helpful.

**Style Examples (adapt to user's language):**
The following are English examples of the TONE. Do NOT use them literally if the user is not speaking English.
- "I can help with that! First, could you tell me [question]?"
- "Happy to help! I just need a bit more info - [question]"
- "Sure thing! Quick question for you - [question]"

**Bad Examples (wrong tone):**
- Asking the question without any lead-in.
- Being too formal or robotic.

Now, ask the following question in a conversational manner in the user's language: "${routingDecision.question}"`,
                  prompt: `Ask this question naturally: ${routingDecision.question}`,
                  onFinish: async (completion) => {
                    if (abortController.signal.aborted) return;
                    await handleStreamCompletion(
                      supabase,
                      assistantMessageId,
                      user!.id,
                      'gemini-2.0-flash',
                      getProviderFromModel('gemini-2.0-flash'),
                      completion,
                      isRegeneration,
                      { original_model: requestData.originalModel || model, token_usage: completion.usage }
                    );
                  }
                });
                clarificationResult.mergeIntoDataStream(dataStream);
                break;

              case 'TEXT_RESPONSE': {
                // Route A: Generate a complete text-based response, using tools conversationally.
                const tools: Record<string, any> = {};
                routingDecision.tools.forEach((toolName: string) => {
                  tools[toolName] = initializeTool(toolName, dataStream);
                });

                // V7: Recalculate context for the specific route
                const systemPrompt = buildSystemPrompt('agent', 'TEXT_RESPONSE', memoryData || undefined);
                const preciseSystemTokens = estimateTokenCount(systemPrompt);
                const preciseRemainingTokens = maxContextTokens - preciseSystemTokens;
                const finalMessages = selectMessagesWithinTokenLimit(
                  messagesWithTokensFinal,
                  preciseRemainingTokens,
                );

                // moonshotai/kimi-k2-instruct ‚Üí moonshotai/Kimi-K2-Instruct (ÎèÑÍµ¨ ÏÇ¨Ïö© Ïãú)
                let toolExecutionModel = (model.startsWith('gemini')) ? 'claude-sonnet-4-20250514' : model;
                if (toolExecutionModel === 'moonshotai/kimi-k2-instruct') {
                  toolExecutionModel = 'moonshotai/Kimi-K2-Instruct';
                }
                const textResponsePromise = streamText({
                  model: providers.languageModel(toolExecutionModel),
                  experimental_transform: [
                    smoothStream({delayInMs: 25}),
                    markdownJoinerTransform(),
                  ],
                  system: systemPrompt,
                  messages: convertMultiModalToMessage(finalMessages), // Convert back for the SDK
                  tools,
                  maxSteps: 20,
                  maxRetries:3,
                  providerOptions,
                  onFinish: async (completion) => {
                    if (abortController.signal.aborted) return;
                    
                    // üîß FIX: ÎèÑÍµ¨Î≥Ñ Í≤∞Í≥º ÏàòÏßë (ÌÜµÌï© Ìï®Ïàò ÏÇ¨Ïö©)
                    const collectedToolResults = collectToolResults(tools, routingDecision.tools);
                    
                    // 2. Increment request count
                    await incrementSuccessfulRequestCount(supabase, user!.id, today, currentRequestCount, isSubscribed);

                    // 3. Generate and stream follow-up questions (Í∞úÏÑ†Îêú Ï†ÑÎûµ Ï†ÅÏö©)
                    const followUpQuestions = await generateFollowUpQuestions(userQuery, completion.text, 'text');
                    
                    const structuredResponse = {
                      response: { 
                        followup_questions: followUpQuestions 
                      }
                    };
                    collectedToolResults.structuredResponse = structuredResponse;
                    
                    // Send as structured_response to match client expectations
                    dataStream.writeMessageAnnotation({
                      type: 'structured_response',
                      data: structuredResponse
                    });
                    
                    // 1. Save main completion to DB (Ïù¥Ï†ú followup question Ìè¨Ìï®)
                    await handleStreamCompletion(
                      supabase,
                      assistantMessageId,
                      user!.id,
                      model,
                      getProviderFromModel(model),
                      completion,
                      isRegeneration,
                      {
                        original_model: requestData.originalModel || model,
                        token_usage: completion.usage,
                        tool_results: collectedToolResults
                      }
                    );

                    // 4. üÜï Smart Memory Update - AI Î∂ÑÏÑù Í∏∞Î∞ò ÏßÄÎä•Ï†Å ÏóÖÎç∞Ïù¥Ìä∏
                    setTimeout(async () => {
                      try {
                        await smartUpdateMemoryBanks(
                          supabase, 
                          user!.id, 
                          chatId, 
                          finalMessages, 
                          userQuery, 
                          completion.text
                        );
                      } catch (error) {
                        console.error('Smart memory update failed:', error);
                      }
                    }, 1000);
                  }
                });

                textResponsePromise.mergeIntoDataStream(dataStream, { sendReasoning: true });
                      break;
                  }
                  
              case 'FILE_RESPONSE': {
                // Route B: A two-step process to reliably generate files.
                const tools: Record<string, any> = {};
                routingDecision.tools.forEach((toolName: string) => {
                  tools[toolName] = initializeTool(toolName, dataStream);
                });

                // Check if tools are needed
                const needsTools = routingDecision.tools.length > 0;

                // üîß FIX: Step 1ÏóêÏÑúÎäî Í∞ÑÎã®Ìïú Ï†ÑÏö© ÌîÑÎ°¨ÌîÑÌä∏ ÏÇ¨Ïö© + ÏÇ¨Ïö©Ïûê Ïñ∏Ïñ¥ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                const userLanguageContext = memoryData ? `\n\n## USER PROFILE CONTEXT\n${memoryData}` : '';
                
                // Check if using DeepSeek or Claude Sonnet models (these may take longer for file generation)
                const isSlowerModel = model.toLowerCase().includes('deepseek') || 
                                     (model.includes('claude') && model.includes('sonnet'));
                
                const systemPromptForFileStep1 = needsTools 
                  ? `You are Chatflix, a friendly and helpful AI assistant. You are in the data collection phase for file generation. Your goal is to use tools to gather information while communicating naturally with the user.

**Core Instruction: ALWAYS respond in the user's language.** Your responses should feel like a real person sending a message.

**Your Task:**
1.  Briefly and conversationally tell the user what you are doing (e.g., searching for information).
2.  Use the necessary tools to collect information.
3.  When finished, let the user know you are ready to create the file.
4.  Do NOT provide detailed explanations in the chat; save that for the file.

**Style Examples (adapt to user's language):**
The following are English examples of the TONE. Do NOT use them literally if the user is not speaking English.
- "Let me look that up for you..."
- "I'll search for the latest info on that..."
- "Alright, I have what I need. Let me put that file together for you."
- "Okay, I'm all set. I'll get that file ready now."

Today's date is ${new Date().toLocaleDateString('en-US', { year: 'numeric', month: 'long', day: 'numeric' })}.
${userLanguageContext}

**IMPORTANT: Always respond in the same language as the user's query.** If a user profile indicates a preferred language, use that language.`
                  : `You are Chatflix, a friendly and helpful AI assistant. You're about to create a file for the user. This is NOT the main response phase; you are just announcing that you're starting the work.

**Core Instruction: ALWAYS respond in the user's language.** Your response should feel like a real person sending a quick confirmation message.

**Your Task:**
- Write 1-2 SHORT, friendly sentences to announce that you're starting to create the file.
- Your tone should be helpful and natural.
- You MUST mention the word "file" (or its equivalent in the user's language).
${isSlowerModel ? `- **IMPORTANT**: Since you're using a ${model.includes('deepseek') ? 'DeepSeek' : 'Claude Sonnet'} model, mention that file generation might take a bit longer but will provide high-quality results.` : ''}

**Style Examples (adapt to user's language):**
The following are English examples of the TONE. Do NOT use them literally if the user is not speaking English.
- "Sure thing! Let me create that file for you."
- "Got it! I'll put together that file right away."
- "Perfect! I'll generate that file for you now."
- "Alright! I'll whip up that file for you."
${isSlowerModel ? `- "I'll create that file for you. It might take a moment as I'm using a high-performance model for better quality!"` : ''}

**Bad Examples (wrong tone):**
- "Generating file." (too robotic)
- "File creation initiated." (too formal)
- "I'll put that together." (doesn't mention "file")

Today's date is ${new Date().toLocaleDateString('en-US', { year: 'numeric', month: 'long', day: 'numeric' })}.
${userLanguageContext}

**IMPORTANT: Always respond in the same language as the user's query.** If a user profile indicates a preferred language, use that language.`;

                const preciseSystemTokensFile = estimateTokenCount(systemPromptForFileStep1);
                const preciseRemainingTokensFile = maxContextTokens - preciseSystemTokensFile;
                const finalMessages = selectMessagesWithinTokenLimit(
                  messagesWithTokensFinal,
                  preciseRemainingTokensFile,
                );
                const finalMessagesConverted = convertMultiModalToMessage(finalMessages);


                if (needsTools) {
                  // Step 1: Execute tools and interact with the user (only if tools are needed)
                // Gemini Î™®Îç∏Î°ú ÎèÑÍµ¨ Ïã§Ìñâ Ïãú Claude Sonnet 4Î°ú ÎåÄÏ≤¥
                let toolExecutionModel = (model.startsWith('gemini')) ? 'claude-sonnet-4-20250514' : model;
                if (toolExecutionModel === 'moonshotai/kimi-k2-instruct') {
                  toolExecutionModel = 'moonshotai/Kimi-K2-Instruct';
                }
                const toolExecutionPromise = streamText({
                  model: providers.languageModel(toolExecutionModel),
                    experimental_transform: [
                      smoothStream({delayInMs: 25}),
                      markdownJoinerTransform(),
                    ],
                    system: systemPromptForFileStep1,
                    messages: finalMessagesConverted,
                    tools,
                    maxSteps: 20, 
                    maxRetries:3,
                    providerOptions,
                    onFinish: async (toolExecutionCompletion) => {
                      if (abortController.signal.aborted) return;
                      
                      // üîß FIX: ÎèÑÍµ¨Î≥Ñ Í≤∞Í≥º ÏàòÏßë (FILE_RESPONSE - ÎèÑÍµ¨ ÏÇ¨Ïö© ÏºÄÏù¥Ïä§, ÌÜµÌï© Ìï®Ïàò ÏÇ¨Ïö©)
                      const collectedToolResults = collectToolResults(tools, routingDecision.tools);
                      
                      await generateFileWithToolResults(collectedToolResults, toolExecutionCompletion, finalMessagesConverted);
                    }
                  });
                  
                  toolExecutionPromise.mergeIntoDataStream(dataStream, { sendReasoning: true });
                } else {
                  // No tools needed - but still provide a brief explanation before file generation
                  const briefExplanationPromise = streamText({
                    model: providers.languageModel('gemini-2.0-flash'),
                    experimental_transform: [
                      smoothStream({delayInMs: 25}),
                      markdownJoinerTransform(),
                    ],
                    // providerOptions,
                    temperature: 0.0,
                    maxTokens: 3000,
                    system: systemPromptForFileStep1, // Re-use the 'file_announcement' prompt
                    messages: finalMessagesConverted,
                    onFinish: async (briefCompletion) => {
                      if (abortController.signal.aborted) return;
                      // Call file generation after brief explanation is complete
                      // briefCompletionÏùÑ Ï†ÑÎã¨ÌïòÏó¨ ÏµúÏ¢Ö Ï†ÄÏû• Ïãú Ìè¨Ìï®ÏãúÌÇ¥
                      await generateFileWithToolResults(null, briefCompletion, finalMessagesConverted);
                    }
                  });
                  
                  briefExplanationPromise.mergeIntoDataStream(dataStream, { sendReasoning: true });
                }

                // Helper function to generate files (extracted to avoid code duplication)
                async function generateFileWithToolResults(toolResults: any, stepCompletion: any, messagesForGeneration: any[]) {
                  // Setup progress tracking
                  const startTime = Date.now();
                  let progressCount = 0;
                  let progressInterval: NodeJS.Timeout | null = null;
                  let isFileGenerationComplete = false;
                  let accumulatedContent = ''; // ÎàÑÏ†ÅÎêú Ïª®ÌÖêÏ∏† Ï†ÄÏû•
                  let sentProgressMessages: string[] = []; // Ï†ÑÏÜ°Îêú ÏßÑÌñâ Î©îÏãúÏßÄÎì§ Ï∂îÏ†Å
                                
                  // Determine the model for file generation (replace claude sonnet, and grok-4 with Gemini 2.5 Pro)
                  let fileGenerationModel = model;
                  if (
                      (model.includes('claude') && model.includes('sonnet')) || 
                      model.toLowerCase().startsWith('grok-4')) {
                    fileGenerationModel = 'gemini-2.5-pro';
                  } 
                  
                  else if (model === 'moonshotai/kimi-k2-instruct') {
                    // üÜï moonshotai/kimi-k2-instructÎäî streamObject Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ gpt-4.1Î°ú ÎåÄÏ≤¥
                    fileGenerationModel = 'gpt-4.1';
                  }

                  // Helper function to generate intermediate progress messages
                  async function generateProgressMessage(progressCount: number, userQuery: string, estimatedTimeElapsed: number, memoryData?: string) {
                    try {
                      // ÏßÑÌñâ Î©îÏãúÏßÄ ÏÉùÏÑ± Ï§ëÎã® Ï≤¥ÌÅ¨
                      if (isFileGenerationComplete) return null;
                      
                      const progressResult = streamText({
                        model: providers.languageModel('gemini-2.0-flash'),
                        experimental_transform: [
                          smoothStream({delayInMs: 25}),
                          markdownJoinerTransform(),
                        ],
                        system: `You are Chatflix, an AI assistant generating a file for the user. This can take some time, so you need to send a brief, natural-sounding waiting message.

**Core Instruction: ALWAYS respond in the user's language.** Your message should sound like a real person sending a quick text.

**User's Request:** ${userQuery}
**Time Elapsed:** About ${estimatedTimeElapsed} seconds.

**Your Task:**
- Send a short, reassuring message (1 sentence).
- Acknowledge that file generation can take time.
- Vary your message each time.

**Message Type Examples (adapt to user's language):**
Rotate between these types of messages. Do NOT use the English text literally if the user speaks another language.
- **Time Expectation:** "Just a heads-up, this file is taking a moment to generate..."
- **Patience Request:** "Thanks for your patience, still working on this file for you."
- **Process Explanation:** "Still getting everything ready for your file..."
- **Reassurance:** "Still here and working on it! Complex files can sometimes take a bit longer."

**Previously Sent Messages:** ${sentProgressMessages.join(', ')}

${memoryData ? `**User Profile Context:**
${memoryData}

**CRITICAL: Respond in the user's preferred language from their profile. If none, use the language of their query.**` : '**IMPORTANT: Always respond in the language of the user\'s query.**'}

Generate a new, different waiting message.`,
                        prompt: `Brief waiting message #${progressCount}`,
                        temperature: 0.8,
                        maxTokens: 50,
                        onFinish: async (completion) => {
                          // Ïù¥Ï§ë Ï≤¥ÌÅ¨: ÏôÑÎ£å Ï≤òÎ¶¨ Ï§ëÏóêÎèÑ Ï§ëÎã® ÏÉÅÌÉú ÌôïÏù∏
                          if (isFileGenerationComplete) return;
                          
                          // Ï†ÑÏÜ°Îêú Î©îÏãúÏßÄ Ï∂îÏ†ÅÏóê Ï∂îÍ∞Ä
                          sentProgressMessages.push(completion.text);
                          
                          // ÏßÑÌñâ Î©îÏãúÏßÄÎ•º ÎàÑÏ†Å Ïª®ÌÖêÏ∏†Ïóê Íµ¨Î∂ÑÏûêÏôÄ Ìï®Íªò Ï∂îÍ∞Ä
                          const separator = accumulatedContent ? '\n\n---\n\n' : '';
                          accumulatedContent += separator + completion.text;
                          
                          // Í∏∞Ï°¥ assistant Î©îÏãúÏßÄÎ•º ÏóÖÎç∞Ïù¥Ìä∏ (Î≥ÑÎèÑ Î©îÏãúÏßÄÎ°ú Ï†ÄÏû•ÌïòÏßÄ ÏïäÏùå)
                          await supabase
                            .from('messages')
                            .update({
                              content: accumulatedContent,
                              model: 'gemini-2.0-flash',
                              host: getProviderFromModel('gemini-2.0-flash'),
                              created_at: new Date().toISOString()
                            })
                            .eq('id', assistantMessageId)
                            .eq('user_id', user!.id);
                        }
                      });

                      // Ïä§Ìä∏Î¶ºÏùÑ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Ï†ÑÏÜ°
                      progressResult.mergeIntoDataStream(dataStream);
                      
                      return progressResult;
                    } catch (error) {
                      return null;
                    }
                  }

                  // Start progress message timer
                  const startProgressUpdates = () => {
                    const sendProgressMessage = async () => {
                      // ÏßÑÌñâ Î©îÏãúÏßÄ ÏÉùÏÑ± ÏûêÏ≤¥Î•º Ï§ëÎã®
                      if (isFileGenerationComplete) return;
                      
                      progressCount++;
                      const elapsedTime = Math.floor((Date.now() - startTime) / 1000);
                      
                      // Îã§Ïãú Ìïú Î≤à Ï≤¥ÌÅ¨ (ÎπÑÎèôÍ∏∞ Ìï®ÏàòÏù¥ÎØÄÎ°ú)
                      if (isFileGenerationComplete) return;
                      
                      const progressResult = await generateProgressMessage(progressCount, userQuery, elapsedTime, memoryData || undefined);
                    };

                    // Send first progress message after 15 seconds
                    setTimeout(sendProgressMessage, 15000);
                    
                    // Then send progress messages every 90-120 seconds (randomized)
                    const scheduleNextProgressMessage = () => {
                      if (isFileGenerationComplete) return;
                      
                      const randomInterval = 90000 + Math.random() * 30000; // 90-120 seconds
                      progressInterval = setTimeout(async () => {
                        await sendProgressMessage();
                        scheduleNextProgressMessage();
                      }, randomInterval);
                    };
                    
                    scheduleNextProgressMessage();
                  };

                  // Start the progress tracking
                  startProgressUpdates();
                  
                  // Handle abort scenarios
                  if (abortController.signal.aborted) {
                    isFileGenerationComplete = true;
                    if (progressInterval) {
                      clearTimeout(progressInterval);
                      progressInterval = null;
                    }
                    return;
                  }
                  
                  // Step 2: Generate the file using the collected results
                  const fileGenerationSystemPrompt = toolResults 
                    ? `${buildSystemPrompt('agent', 'FILE_RESPONSE', memoryData || undefined)}

Tool results available:
<tool_results>
${JSON.stringify(toolResults, null, 2)}
</tool_results>

üö® **CRITICAL FILE GENERATION RULE** üö®
For ALL programming/code files (js, ts, py, java, cpp, html, css, json, xml, yaml, etc.), the file content MUST start with the appropriate code block syntax:

\`\`\`language
[your code here]
\`\`\`

This is MANDATORY for proper rendering. Examples:
- JavaScript/TypeScript: \`\`\`javascript or \`\`\`typescript
- Python: \`\`\`python
- HTML: \`\`\`html
- CSS: \`\`\`css
- JSON: \`\`\`json
- Any code file: \`\`\`[language]

**NEVER generate bare code without code block syntax - this causes rendering issues!**`
                    : `${buildSystemPrompt('agent', 'FILE_RESPONSE', memoryData || undefined)}
${hasImage ? `\n- An image has been provided. You can analyze it to inform your file creation.` : ''}
${hasFile ? `\n- A file has been provided. You can read its content to inform your file creation.` : ''}

üö® **CRITICAL FILE GENERATION RULE** üö®
For ALL programming/code files (js, ts, py, java, cpp, html, css, json, xml, yaml, etc.), the file content MUST start with the appropriate code block syntax:

\`\`\`language
[your code here]
\`\`\`

This is MANDATORY for proper rendering. Examples:
- JavaScript/TypeScript: \`\`\`javascript or \`\`\`typescript
- Python: \`\`\`python
- HTML: \`\`\`html
- CSS: \`\`\`css
- JSON: \`\`\`json
- Any code file: \`\`\`[language]

**NEVER generate bare code without code block syntax - this causes rendering issues!**`;

                  const fileGenerationResult = await streamObject({
                    model: providers.languageModel(fileGenerationModel),
                    system: fileGenerationSystemPrompt,
                    messages: messagesForGeneration,
                    schema: z.object({
                      response: z.object({
                        description: z.string().describe('A casual, friendly sentence to present the files to the user in their language. Sound like a friend handing over completed work. Examples: "All set! Here are your files." or "Perfect! Got everything ready for you." or "Here you go - all done!" Keep it relaxed and casual.'),
                        files: z.array(z.object({
                            name: z.string().describe('Name of the file with appropriate extension.'),
                            content: z.string().describe('COMPREHENSIVE content of the file with ALL details, explanations, and information. This should contain the actual answer to the user\'s request. Format appropriately for the file type. **CRITICAL**: For code files, ALWAYS start with proper code block syntax (```language). Never generate bare code without markdown code blocks!'),
                          })
                        ).describe("Array of files containing ALL the detailed content and answers."),
                      })
                    })
                  });

                  // Stream partial file object to the client for a responsive UI
                  (async () => {
                    let firstPartialReceived = false;
                    for await (const partial of fileGenerationResult.partialObjectStream) {
                      if (abortController.signal.aborted) break;
                      
                      // Ï≤´ Î≤àÏß∏ Ïä§Ìä∏Î¶ºÏù¥ ÏãúÏûëÎêòÎ©¥ ÏßÑÌñâ Î©îÏãúÏßÄ Ï§ëÎã®
                      if (!firstPartialReceived) {
                        firstPartialReceived = true;
                        isFileGenerationComplete = true;
                        if (progressInterval) {
                          clearTimeout(progressInterval);
                          progressInterval = null;
                        }
                      }
                      
                      dataStream.writeMessageAnnotation({ type: 'structured_response_progress', data: JSON.parse(JSON.stringify(partial)) });
                    }
                  })();
                  
                  const finalFileObjectFromStream = await fileGenerationResult.object;
                  const fileDescription = finalFileObjectFromStream.response.description || "Here are the files you requested.";

                  // Mark file generation as complete and cleanup progress tracking
                  isFileGenerationComplete = true;
                  if (progressInterval) {
                    clearTimeout(progressInterval);
                    progressInterval = null;
                  }

                  // Start with the base object and add follow-up questions to it.
                  const finalFileObject: any = finalFileObjectFromStream;

                  // Send final structured response and follow-up questions (Í∞úÏÑ†Îêú Ï†ÑÎûµ Ï†ÅÏö©)
                  const followUpQuestions = await generateFollowUpQuestions(userQuery, fileDescription, 'file');
                  finalFileObject.response.followup_questions = followUpQuestions;
                  
                  dataStream.writeMessageAnnotation({
                    type: 'structured_response',
                    data: finalFileObject
                  });

                  // Manually construct a 'completion' object for saving
                  let finalCompletionForDB;
                  
                  if (stepCompletion) {
                    // Case: Tools were used OR brief explanation was provided
                    const [
                      stepUsage,
                      fileUsage,
                      finishReason
                    ] = await Promise.all([
                      stepCompletion.usage,
                      fileGenerationResult.usage,
                      stepCompletion.finishReason,
                    ]);

                    // Ï≤´ Î≤àÏß∏ Îã®Í≥Ñ ÌÖçÏä§Ìä∏Î•º ÎàÑÏ†Å Ïª®ÌÖêÏ∏†Ïóê Ï∂îÍ∞Ä
                    if (stepCompletion.text) {
                      const separator = accumulatedContent ? '\n\n---\n\n' : '';
                      accumulatedContent += separator + stepCompletion.text;
                    }

                    // üîß FIX: Ïù¥Ï†Ñ Îã®Í≥Ñ(ÎèÑÍµ¨ Ïã§Ìñâ ÎòêÎäî Í∞ÑÎã® ÏÑ§Î™Ö)Ïùò ÌÖçÏä§Ìä∏Îßå Ìè¨Ìï®
                    // fileDescriptionÏùÄ Î≥ÑÎèÑÎ°ú Ï∂îÍ∞ÄÌïòÏßÄ ÏïäÏùå (Ïù¥ÎØ∏ ÏßÑÌñâ Î©îÏãúÏßÄÏóê Ìè¨Ìï®Îê®)
                    const combinedText = accumulatedContent || fileDescription;

                    finalCompletionForDB = {
                      text: combinedText,
                      usage: {
                        promptTokens: (stepUsage.promptTokens || 0) + (fileUsage.promptTokens || 0),
                        completionTokens: (stepUsage.completionTokens || 0) + (fileUsage.completionTokens || 0),
                        totalTokens: (stepUsage.totalTokens || 0) + (fileUsage.totalTokens || 0),
                      },
                      finishReason: finishReason
                    };
                  } else {
                    // Case: No tools were used but brief explanation was provided
                    if (stepCompletion && stepCompletion.text) {
                      const separator = accumulatedContent ? '\n\n---\n\n' : '';
                      accumulatedContent += separator + stepCompletion.text;
                    }
                    
                    const fileUsage = await fileGenerationResult.usage;
                    const finalText = accumulatedContent || fileDescription;
                    
                    finalCompletionForDB = {
                      text: finalText,
                      usage: fileUsage,
                      finishReason: 'stop'
                    };
                  }
                  
                  // üîß FIX: ÎèÑÍµ¨ Í≤∞Í≥º Ïò¨Î∞îÎ•¥Í≤å Ï≤òÎ¶¨
                  let finalToolResults: any = {
                    structuredResponse: finalFileObject
                  };
                  
                  // toolResultsÍ∞Ä Ïù¥ÎØ∏ ÏàòÏßëÎêú ÎèÑÍµ¨ Í≤∞Í≥ºÎùºÎ©¥ Í∑∏Í≤ÉÏùÑ ÏÇ¨Ïö©
                  if (toolResults && typeof toolResults === 'object' && !Array.isArray(toolResults)) {
                    // toolResultsÏóê Ïù¥ÎØ∏ ÏÇ¨Ïö©Ïûê Ï†ïÏùò Í≤∞Í≥ºÎì§Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎã§Î©¥ Î≥ëÌï©
                    finalToolResults = {
                      ...toolResults,
                      structuredResponse: finalFileObject
                    };
                  }

                  // Ïù¥ÎØ∏ finalCompletionForDB.textÏóê Î™®Îì† ÎàÑÏ†Å Ïª®ÌÖêÏ∏†Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏùå

                  // Finalize the process (save to DB, increment count, update memory)
                  await handleStreamCompletion(
                    supabase,
                    assistantMessageId,
                    user!.id,
                    fileGenerationModel,
                    getProviderFromModel(fileGenerationModel),
                    finalCompletionForDB as any,
                    isRegeneration,
                    { 
                      original_model: requestData.originalModel || model,
                      token_usage: finalCompletionForDB.usage,
                      tool_results: finalToolResults
                    }
                  );

                  await incrementSuccessfulRequestCount(supabase, user!.id, today, currentRequestCount, isSubscribed);
                  
                  // üÜï Smart Memory Update for file generation
                  setTimeout(async () => {
                    try {
                      await smartUpdateMemoryBanks(
                        supabase, 
                        user!.id, 
                        chatId, 
                        finalMessages, 
                        userQuery, 
                        fileDescription
                      );
                    } catch (error) {
                      console.error('Smart memory update failed:', error);
                    }
                  }, 1000);
                }
                
                break;
              }
            }
            // =================================================================
            // END: NEW V6 LOGIC
            // =================================================================

          } else {
            // ÏùºÎ∞ò Ï±ÑÌåÖ ÌùêÎ¶Ñ - ÏõêÎûò ÏΩîÎìú ÏÇ¨Ïö©Ïóê ÌÜ†ÌÅ∞ Ï†úÌïú ÏµúÏ†ÅÌôî Ï∂îÍ∞Ä
            //  Ïù¥ÎØ∏ Í≥ÑÏÇ∞Îêú ÏãúÏä§ÌÖú ÌÜ†ÌÅ∞ Ïû¨ÏÇ¨Ïö©

            const optimizedMessages = selectMessagesWithinTokenLimit(
              messagesWithTokens, 
              remainingTokens,
            );

            const messages = convertMultiModalToMessage(optimizedMessages);

            const result = streamText({
              model: providers.languageModel(model),
              experimental_transform: [
                smoothStream({delayInMs: 25}),
                markdownJoinerTransform(),
              ],
              system: currentSystemPrompt, // Use the 'regular' prompt calculated earlier
              messages: messages,
              // temperature: 0.7,
              // maxTokens: 20000,
              providerOptions: providerOptions,
              onFinish: async (completion) => {
                if (abortController.signal.aborted) return;

                // üÜï Ïã§Ï†ú ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï∂îÏ∂ú Î∞è Î°úÍπÖ
                const actualTokenUsage = completion.usage;
                // if (actualTokenUsage) {
                //   console.log('üî¢ [TOKEN USAGE] Regular mode actual tokens:', {
                //     promptTokens: actualTokenUsage.promptTokens,
                //     completionTokens: actualTokenUsage.completionTokens,
                //     totalTokens: actualTokenUsage.totalTokens,
                //     model: model,
                //     messageId: assistantMessageId
                //   });
                // }

                await handleStreamCompletion(
                  supabase,
                  assistantMessageId,
                  user!.id,
                  model,
                  getProviderFromModel(model),
                  completion,
                  isRegeneration,
                  { 
                    original_model: requestData.originalModel || model,
                    token_usage: actualTokenUsage // üÜï Ïã§Ï†ú ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï∂îÍ∞Ä
                  }
                );

                // Increment daily request count only on successful, non-aborted completion
                if (!abortController.signal.aborted) {
                  await incrementSuccessfulRequestCount(
                    supabase,
                    user!.id,
                    today,
                    currentRequestCount,
                    isSubscribed
                  );
                }

                // üÜï Smart Memory Update for regular chat
                if (chatId && !abortController.signal.aborted) {
                  // AIÏùò ÏùëÎãµÍ≥º ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï§ÄÎπÑ
                  const userMessage = typeof processedLastMessage.content === 'string' 
                    ? processedLastMessage.content 
                    : JSON.stringify(processedLastMessage.content);
                  const aiMessage = completion.text;
                  
                  // 1Ï¥à ÎîúÎ†àÏù¥Î°ú Smart ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìñâ
                  setTimeout(async () => {
                    try {
                      await smartUpdateMemoryBanks(
                        supabase, 
                        user!.id, 
                        chatId, 
                        optimizedMessages, 
                        userMessage, 
                        aiMessage
                      );
                    } catch (error) {
                      console.error('Smart memory update failed:', error);
                    }
                  }, 1000);
                }
              }
            });

            result.mergeIntoDataStream(dataStream, {
              sendReasoning: true
            });

          }

      }
          });
}


